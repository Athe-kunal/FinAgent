{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Agent, Crew\n",
    "from crewai_tools import JSONSearchTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_earnings_transcript\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from crewai_tools import JSONSearchTool\n",
    "\n",
    "base_dir = \"DATA\"\n",
    "os.makedirs(base_dir,exist_ok=True)\n",
    "def clean_speakers(speaker):\n",
    "    speaker = re.sub(\"\\n\", \"\", speaker)\n",
    "    speaker = re.sub(\":\", \"\", speaker)\n",
    "    return speaker\n",
    "\n",
    "quarter = \"Q1\"\n",
    "ticker = \"AAPL\"\n",
    "year = \"2023\"\n",
    "\n",
    "def get_save_earnings_data(quarter:str,ticker:str,year:str):\n",
    "    ticker_path = os.path.join(base_dir,ticker)\n",
    "    os.makedirs(ticker_path,exist_ok=True)\n",
    "    ticker_year = os.path.join(ticker_path,year)\n",
    "    os.makedirs(ticker_year,exist_ok=True)\n",
    "    ticker_year_quarter = os.path.join(ticker_year,quarter)\n",
    "    os.makedirs(ticker_year_quarter,exist_ok=True)\n",
    "\n",
    "    resp_dict = get_earnings_transcript(quarter,ticker,year)\n",
    "\n",
    "    content = resp_dict[\"content\"]\n",
    "    pattern = re.compile(r\"\\n(.*?):\")\n",
    "    matches = pattern.finditer(content)\n",
    "\n",
    "    speakers_list = []\n",
    "    ranges = []\n",
    "    for match_ in matches:\n",
    "        span_range = match_.span()\n",
    "        ranges.append(span_range)\n",
    "        speakers_list.append(match_.group())\n",
    "    speakers_list = [clean_speakers(sl) for sl in speakers_list]\n",
    "\n",
    "    earning_calls_docs = []\n",
    "    for idx, speaker in enumerate(speakers_list[:-1]):\n",
    "        start_range = ranges[idx][1]\n",
    "        end_range = ranges[idx + 1][0]\n",
    "        speaker_text = content[start_range + 1 : end_range]\n",
    "\n",
    "        earning_calls_docs.append({\"speaker\":speaker,\"quarter\":quarter,\"speaker_text\":f\"{speaker}: {speaker_text}\"})\n",
    "\n",
    "    earning_calls_docs.append({\"speaker\": speakers_list[-1], \"quarter\": quarter,'speaker_text':content[ranges[-1][1] :]})\n",
    "    json_path = os.path.join(ticker_year_quarter,f'{quarter}.json')\n",
    "    with open(json_path, 'w') as file:\n",
    "        json.dump(earning_calls_docs, file, indent=4)\n",
    "    json_tool = JSONSearchTool(json_path=json_path)\n",
    "    return json_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings Call Q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:20<00:00, 20.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from tenacity import RetryError\n",
    "from crewai_tools import JSONSearchTool  # Updated import path\n",
    "\n",
    "print(\"Earnings Call Q1\")\n",
    "\n",
    "def get_quarter_tool(quarters,ticker,year):\n",
    "    earnings_calls_tools = []\n",
    "    for quarter in quarters:\n",
    "        try:\n",
    "            curr_quarter_tool = get_save_earnings_data(quarter,ticker,year)\n",
    "            earnings_calls_tools.append(curr_quarter_tool)\n",
    "        except RetryError:\n",
    "            print(f\"Don't have the data for {quarter}\")\n",
    "    return earnings_calls_tools\n",
    "earnings_calls_tools = get_quarter_tool([\"Q1\",\"Q2\",\"Q3\",\"Q4\"],ticker,year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Scraping\n",
      "Scraped\n",
      "Started Extracting\n",
      "Extracted\n"
     ]
    }
   ],
   "source": [
    "from src.secData import sec_main\n",
    "sec_docs,sec_form_names = sec_main(ticker=ticker,year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec_filings,sec_name in zip(sec_docs,sec_form_names):\n",
    "    report_year = sec_filings[-2].split(\"-\")[0]\n",
    "    if sec_name.startswith(\"10-Q\"):\n",
    "        curr_quarter = sec_name.split(\"-\")[-1]\n",
    "        save_path = os.path.join(base_dir,ticker,report_year,curr_quarter)\n",
    "    else:\n",
    "        save_path = os.path.join(base_dir,ticker,report_year)\n",
    "    os.makedirs(save_path,exist_ok=True)\n",
    "    with open(os.path.join(save_path,f'{sec_name}.json'), 'w') as file:\n",
    "        json.dump(sec_filings[-1], file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings Call Q1\n",
      "Earnings Call Q2\n",
      "Earnings Call Q3\n",
      "Earnings Call Q4\n",
      "SEC\n",
      "Started Scraping\n",
      "Scraped\n",
      "Started Extracting\n",
      "Extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athekunal/Finance Data/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.vectorDatabaseDocker import create_database\n",
    "\n",
    "(\n",
    "        qdrant_client,\n",
    "        encoder,\n",
    "        speakers_list_1,\n",
    "        speakers_list_2,\n",
    "        speakers_list_3,\n",
    "        speakers_list_4,\n",
    "        sec_form_names,\n",
    "        earnings_call_quarter_vals,\n",
    "    ) = create_database(ticker=ticker, year=int(float(year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
